{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import *\n",
    "from processing import *\n",
    "from implementations import *\n",
    "from feature_expansion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30) (568238,)\n"
     ]
    }
   ],
   "source": [
    "y_test, x_test, id_test = load_csv_data(\"../data/test.csv\")\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.genfromtxt(\"../data/train.csv\",\n",
    "              delimiter=',',\n",
    "              encoding='UTF-8-sig',\n",
    "              dtype=None,\n",
    "              names=True).dtype.names[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt')\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "16\n",
      "19\n",
      "20\n",
      "21\n",
      "23\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "x_train_cleaned, _, _ = standardize(clean_data(x_train, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset depending on the Pri_Jet_number feature which can take values 0, 1, 2 or 3. Since the number values that are equal to 3 is really small, we will combine it with the values which have 2 so we will have a 3 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "We will now do feature engineering to increase the results we will have. We do degree root transformation, polynomial transformation, logarithmic transformation and reciprocical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07398883  0.09429484  0.61533879 -0.17287391 -0.47835472  0.91872062\n",
      " -0.45853853 -0.45404716 -0.01547287  1.7430674  -0.4707713  -0.47287028\n",
      " -0.48636696 -0.12030909 -0.47714724 -0.4843244   0.09396758 -0.46297347\n",
      " -0.51586555 -0.29876766 -0.49174983  2.43113838 -0.46605423  0.27236973\n",
      " -0.46436151 -0.48361346  0.03117869 -0.47463072 -0.51655392  0.79217235]\n",
      "(250000, 0)\n",
      "1\n",
      "2\n",
      "3\n",
      "(250000, 87)\n"
     ]
    }
   ],
   "source": [
    "x_train_finished = build_new_x(x_train_cleaned, features)\n",
    "x_train_finished = np.insert(x_train_finished, [0], np.ones((x_train.shape[0],1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "  1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "  1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "  3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      " -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "  4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = clean_data(x_train, features)\n",
    "print(np.isnan(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "22\n",
      "29\n",
      "18 99913\n"
     ]
    }
   ],
   "source": [
    "X_0, y_0, X_1, y_1, X_23, y_23 = pre_process_data(x_train, y_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77544, 155)\n"
     ]
    }
   ],
   "source": [
    "print(X_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51004\n"
     ]
    }
   ],
   "source": [
    "weight, loss = least_squares(y_train, x_train_cleaned)\n",
    "print(compute_mse_loss(y_train, x_train_cleaned, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509928\n"
     ]
    }
   ],
   "source": [
    "weight, loss = ridge_regression(y_train, x_train_cleaned, 10)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.215536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, loss = mean_squared_error_gd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, loss = mean_squared_error_sgd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.995336631273058\n",
      "Current iteration=100, loss=1.0491363661898327\n",
      "Current iteration=200, loss=0.27006069549426187\n",
      "Current iteration=300, loss=-0.37199300383762574\n",
      "Current iteration=400, loss=-0.8974960829542113\n",
      "Current iteration=500, loss=-1.3222079469218488\n",
      "Current iteration=600, loss=-1.661546133055785\n",
      "Current iteration=700, loss=-1.9325063200387396\n",
      "Current iteration=800, loss=-2.15229341612981\n",
      "Current iteration=900, loss=-2.335662630699634\n"
     ]
    }
   ],
   "source": [
    "w, l = logistic_regression(y_train, x_train_cleaned, np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.995336631273058\n",
      "Current iteration=100, loss=0.9989272803604264\n",
      "Current iteration=200, loss=0.20757768961068648\n",
      "Current iteration=300, loss=-0.41169620130994156\n",
      "Current iteration=400, loss=-0.8829184065559729\n",
      "Current iteration=500, loss=-1.2271743752375226\n",
      "Current iteration=600, loss=-1.4682340429038723\n",
      "Current iteration=700, loss=-1.632840431601386\n",
      "Current iteration=800, loss=-1.745383871074444\n",
      "Current iteration=900, loss=-1.8236927657875832\n"
     ]
    }
   ],
   "source": [
    "w, l = reg_logistic_regression(y_train, x_train_cleaned, 0.2,  np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d061a9c1203fb1e15f0d5cc124c74443901699d26ef37c9a635c302b72067f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
