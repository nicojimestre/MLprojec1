{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "from helper import load_csv_data\n",
    "from processing import *\n",
    "from implementations import *\n",
    "from feature_expansion import *\n",
    "from crossvalidation import *\n",
    "from metrics import f1_score, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "y_test , x_test , id_test  = load_csv_data(\"../data/test.csv\")\n",
    "# print('train data shape: ', x_train.shape, y_train.shape)\n",
    "# print('test  data shape: ', x_test.shape, y_test.shape)\n",
    "with open('../col_name.json', 'r') as file:\n",
    "    features = json.load(file)['col_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned = standardize(clean_data(x_train, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset depending on the Pri_Jet_number feature which can take values 0, 1, 2 or 3. Since the number values that are equal to 3 is really small, we will combine it with the values which have 2 so we will have a 3 subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "We will now do feature engineering to increase the results we will have. We do degree root transformation, polynomial transformation, logarithmic transformation and reciprocical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_finished = build_new_x(x_train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline\n",
    "\n",
    "cross validation pipeline. we will use 5-fold cross validation for choosing the optimal parameters.\n",
    "below is the list of models we will use throughout this process.\n",
    "\n",
    "1. least_squares (no parameter tuning needed.)\n",
    "2. ridge LS \n",
    "3. mse_gd\n",
    "4. mse_sgd\n",
    "5. logistic\n",
    "6. reg_logistic\n",
    "\n",
    "we will optimise the weigth based on the mse loss. But the selection process will be based on observing F1 score on validation set.\n",
    "\n",
    "To see the effect of data manipulation, we will try 3 sets of data.\n",
    "1. cleaned data\n",
    "2. standardized data\n",
    "3. feature-engineered data\n",
    "\n",
    "From this process, we would expect the 3rd trial would give us the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "class HyperParameterTuner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        model_name: str,\n",
    "        num_folds: int=5,\n",
    "        num_seed: int=0,\n",
    "        max_iter: int=10\n",
    "    ):\n",
    "\n",
    "        available_models = {\n",
    "            'least_squares': least_squares,\n",
    "            'ridge': ridge_regression,\n",
    "            'mse_gd': mean_squared_error_gd,\n",
    "            'mse_sgd': mean_squared_error_sgd,\n",
    "            'logistic': logistic_regression,\n",
    "            'reg_logistic': reg_logistic_regression\n",
    "        }\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = available_models[model_name]\n",
    "        self.model_name = model_name\n",
    "        self.num_folds = num_folds\n",
    "        self.num_seed  = num_seed\n",
    "        self.max_iter  = max_iter\n",
    "        \n",
    "        # build k_indices\n",
    "        np.random.seed(self.num_seed)\n",
    "        self.build_k_indices()\n",
    "\n",
    "        # get model params given model specs.\n",
    "        model_parameters = {\n",
    "            'least_squares': {}, \n",
    "            'ridge'        : {'lambda_': None},\n",
    "            'mse_gd'       : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'mse_sgd'      : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'logistic'     : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'reg_logistic' : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None, 'lambda_': None},\n",
    "        }\n",
    "        self.hyp_params = model_parameters[model_name]\n",
    "\n",
    "    def tune_(self) -> Tuple[list, float]:\n",
    "        \"\"\"\n",
    "        hyperparameter tuning done by grid search.\n",
    "        best parameters are found by finding the maximum f1 scores.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas = np.logspace(-15,    2, 50)\n",
    "        gammas  = np.logspace(-6 ,-0.25, 50)     \n",
    "\n",
    "        f1_scores = np.array([])\n",
    "        params = self.hyp_params\n",
    "        # cross validation\n",
    "        if self.model_name == 'least_squares':\n",
    "            results = np.array([[k] + self.cross_validation_per_k(k, self.hyp_params)[-1] for k in range(self.num_folds)])\n",
    "            return results[np.argmax(results[:,-1])]   \n",
    "        \n",
    "        elif self.model_name == 'reg_logistic':\n",
    "            lambda_and_gammas = product(gammas, lambdas)\n",
    "            for i, (gamma, lambda_) in enumerate(lambda_and_gammas):\n",
    "                params['gamma'], params['lambda_'] = gamma, lambda_\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                \n",
    "                print(f'Progress {i}/{50*50}, lambda_: {lambda_},  gamma: {gamma}, f1: {np.round(np.mean(results, axis=0)[-1], 4)}')            \n",
    "                       \n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = lambda_and_gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'params' :best_params, 'f1_score' :best_f1}     \n",
    "\n",
    "        elif self.model_name == 'ridge':\n",
    "            for i, lambda_ in enumerate(lambdas):\n",
    "                params['lambda_'] =lambda_\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                print(f'Progress {i}/{len(lambdas)}, lambda_: {lambda_}, f1: {np.round(np.mean(results, axis=0)[-1], 4)}')            \n",
    "            \n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = lambdas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'lambda_' :best_params, 'f1_score' :best_f1}     \n",
    "        else:\n",
    "            for i, gamma in enumerate(gammas):\n",
    "                \n",
    "                params['gamma'] = gamma\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                print(f'Progress {i}/{len(gammas)}, gamma :{gamma} f1: {np.round(np.mean(results, axis=0)[-1], 4)}')\n",
    "            # set nan values to 0.\n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            \n",
    "            # get optimum index\n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'gamma' :best_params, 'f1_score' :best_f1}\n",
    "        \n",
    "    def cross_validation_per_k(self, k: int, params: dict) -> list:\n",
    "        \"\"\"return the loss of given model.\"\"\"\n",
    "        # get k'th subgroup in test, others in train\n",
    "        tr_indices, te_indices = self.k_indices[~(np.arange(self.k_indices.shape[0]) == k)].reshape(-1),\\\n",
    "                                 self.k_indices[k]\n",
    "        \n",
    "        # split the data based on train and validation indices\n",
    "        y_trn, y_val = self.y[tr_indices], self.y[te_indices]\n",
    "        x_trn, x_val = self.x[tr_indices], self.x[te_indices]\n",
    "\n",
    "        # run the model\n",
    "        params['tx'], params['y'] = x_trn, y_trn\n",
    "        w, _ = self.model(**params)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        loss_trn = np.sqrt(mse_loss(y_trn, x_trn, w))\n",
    "        loss_val = np.sqrt(mse_loss(y_val, x_val, w))\n",
    "        \n",
    "        # get validation f1-score\n",
    "        y_pred = get_classification_pred(x_val, w)\n",
    "        f1_val = f1_score(y_val, y_pred)\n",
    "        return [loss_trn, loss_val, f1_val]\n",
    "\n",
    "    def build_k_indices(self):\n",
    "        \"\"\"\n",
    "        build k indices for k-fold.\n",
    "        Args:\n",
    "            y:      shape=(N,)\n",
    "            k_fold: K in K-fold, i.e. the fold num\n",
    "            seed:   the random seed\n",
    "\n",
    "        Returns:\n",
    "            A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "        \"\"\"\n",
    "        num_row  = self.y.shape[0]\n",
    "        interval = int(num_row / self.num_folds)\n",
    "        indices  = np.random.permutation(num_row)\n",
    "\n",
    "        self.k_indices = np.array([indices[k * interval : (k + 1) * interval] for k in range(self.num_folds)])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/50, gamma :1e-06 f1: 0.44\n",
      "Progress 1/50, gamma :1.3102281887548672e-06 f1: 0.44\n",
      "Progress 2/50, gamma :1.7166979066078603e-06 f1: 0.44\n",
      "Progress 3/50, gamma :2.2492659888140894e-06 f1: 0.44\n",
      "Progress 4/50, gamma :2.94705170255181e-06 f1: 0.44\n",
      "Progress 5/50, gamma :3.861310214401406e-06 f1: 0.44\n",
      "Progress 6/50, gamma :5.059197488435822e-06 f1: 0.44\n",
      "Progress 7/50, gamma :6.628703161826441e-06 f1: 0.44\n",
      "Progress 8/50, gamma :8.68511373751352e-06 f1: 0.44\n",
      "Progress 9/50, gamma :1.1379480841432356e-05 f1: 0.44\n",
      "Progress 10/50, gamma :1.490971657184063e-05 f1: 0.44\n",
      "Progress 11/50, gamma :1.9535130938771177e-05 f1: 0.44\n",
      "Progress 12/50, gamma :2.559547922699533e-05 f1: 0.44\n",
      "Progress 13/50, gamma :3.353591838789893e-05 f1: 0.44\n",
      "Progress 14/50, gamma :4.393970560760795e-05 f1: 0.44\n",
      "Progress 15/50, gamma :5.757104089267825e-05 f1: 0.44\n",
      "Progress 16/50, gamma :7.543120063354622e-05 f1: 0.4399\n",
      "Progress 17/50, gamma :9.883208538169628e-05 f1: 0.4399\n",
      "Progress 18/50, gamma :0.0001294925842205263 f1: 0.4399\n",
      "Progress 19/50, gamma :0.0001696648340804473 f1: 0.4399\n",
      "Progress 20/50, gamma :0.00022229964825261955 f1: 0.4399\n",
      "Progress 21/50, gamma :0.00029126326549087385 f1: 0.4398\n",
      "Progress 22/50, gamma :0.0003816213407949357 f1: 0.4398\n",
      "Progress 23/50, gamma :0.0005000110381399525 f1: 0.4398\n",
      "Progress 24/50, gamma :0.0006551285568595509 f1: 0.4397\n",
      "Progress 25/50, gamma :0.0008583679024556795 f1: 0.4394\n",
      "Progress 26/50, gamma :0.0011246578221198195 f1: 0.4393\n",
      "Progress 27/50, gamma :0.0014735583812450464 f1: 0.4392\n",
      "Progress 28/50, gamma :0.0019306977288832516 f1: 0.4389\n",
      "Progress 29/50, gamma :0.0025296545883478384 f1: 0.4383\n",
      "Progress 30/50, gamma :0.003314424749466428 f1: 0.4376\n",
      "Progress 31/50, gamma :0.004342652736257703 f1: 0.4367\n",
      "Progress 32/50, gamma :0.005689866029018299 f1: 0.436\n",
      "Progress 33/50, gamma :0.007455022861458495 f1: 0.4344\n",
      "Progress 34/50, gamma :0.009767781100894893 f1: 0.4327\n",
      "Progress 35/50, gamma :0.012798022139979551 f1: 0.4309\n",
      "Progress 36/50, gamma :0.0167683293681101 f1: 0.4295\n",
      "Progress 37/50, gamma :0.021970337816423945 f1: 0.427\n",
      "Progress 38/50, gamma :0.028786155923545713 f1: 0.4246\n",
      "Progress 39/50, gamma :0.03771643293692249 f1: 0.4238\n",
      "Progress 40/50, gamma :0.04941713361323838 f1: 0.428\n",
      "Progress 41/50, gamma :0.06474772146753059 f1: 0.4388\n",
      "Progress 42/50, gamma :0.08483428982440726 f1: 0.4557\n",
      "Progress 43/50, gamma :0.1111522779009386 f1: 0.4764\n",
      "Progress 44/50, gamma :0.14563484775012445 f1: 0.5001\n",
      "Progress 45/50, gamma :0.1908148827872364 f1: 0.5275\n",
      "Progress 46/50, gamma :0.25001103826179305 f1: 0.5621\n",
      "Progress 47/50, gamma :0.32757150983047295 f1: 0.4777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:133: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:133: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(t) / (1 + np.exp(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 48/50, gamma :0.42919342601287785 f1: 0.2401\n",
      "Progress 49/50, gamma :0.5623413251903491 f1: 0.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.25001103826179305, 'f1_score': 0.562109789330598}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'mse_gd', max_iter=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200000,) (200000,29) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [142], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tuner \u001b[39m=\u001b[39m HyperParameterTuner(x_train_cleaned, y_train, \u001b[39m'\u001b[39m\u001b[39mmse_sgd\u001b[39m\u001b[39m'\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m tuner\u001b[39m.\u001b[39;49mtune_()\n",
      "Cell \u001b[0;32mIn [141], line 92\u001b[0m, in \u001b[0;36mHyperParameterTuner.tune_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m i, gamma \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(gammas):\n\u001b[1;32m     91\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gamma\n\u001b[0;32m---> 92\u001b[0m     results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_validation_per_k(k, params) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_folds)])\n\u001b[1;32m     93\u001b[0m     f1_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(f1_scores, np\u001b[39m.\u001b[39mmean(results, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     94\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProgress \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(gammas)\u001b[39m}\u001b[39;00m\u001b[39m, gamma :\u001b[39m\u001b[39m{\u001b[39;00mgamma\u001b[39m}\u001b[39;00m\u001b[39m f1: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mround(np\u001b[39m.\u001b[39mmean(results, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [141], line 92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m i, gamma \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(gammas):\n\u001b[1;32m     91\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gamma\n\u001b[0;32m---> 92\u001b[0m     results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_validation_per_k(k, params) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_folds)])\n\u001b[1;32m     93\u001b[0m     f1_scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(f1_scores, np\u001b[39m.\u001b[39mmean(results, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     94\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProgress \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(gammas)\u001b[39m}\u001b[39;00m\u001b[39m, gamma :\u001b[39m\u001b[39m{\u001b[39;00mgamma\u001b[39m}\u001b[39;00m\u001b[39m f1: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mround(np\u001b[39m.\u001b[39mmean(results, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [141], line 115\u001b[0m, in \u001b[0;36mHyperParameterTuner.cross_validation_per_k\u001b[0;34m(self, k, params)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m# run the model\u001b[39;00m\n\u001b[1;32m    114\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mtx\u001b[39m\u001b[39m'\u001b[39m], params[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m x_trn, y_trn\n\u001b[0;32m--> 115\u001b[0m w, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    117\u001b[0m \u001b[39m# calculate the loss for train and test data\u001b[39;00m\n\u001b[1;32m    118\u001b[0m loss_trn \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mse_loss(y_trn, x_trn, w))\n",
      "File \u001b[0;32m~/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:119\u001b[0m, in \u001b[0;36mmean_squared_error_sgd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# update weight\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient\n\u001b[0;32m--> 119\u001b[0m     losses\u001b[39m.\u001b[39mappend(mse_loss(y, tx, w))\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m w, losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/PycharmProjects/MLprojec1/scripts/metrics.py:12\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmse_loss\u001b[39m(y: np\u001b[39m.\u001b[39mndarray, tx: np\u001b[39m.\u001b[39mndarray, w: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    calculates mse loss of categorical target y\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     error \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39;49m tx \u001b[39m@\u001b[39;49m w\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(error\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200000,) (200000,29) "
     ]
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'mse_sgd', max_iter=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/50, lambda_: 1e-15, f1: 0.6648\n",
      "Progress 1/50, lambda_: 2.222996482526191e-15, f1: 0.6647\n",
      "Progress 2/50, lambda_: 4.9417133613238385e-15, f1: 0.6647\n",
      "Progress 3/50, lambda_: 1.0985411419875573e-14, f1: 0.6648\n",
      "Progress 4/50, lambda_: 2.4420530945486547e-14, f1: 0.6647\n",
      "Progress 5/50, lambda_: 5.4286754393238596e-14, f1: 0.6647\n",
      "Progress 6/50, lambda_: 1.2067926406393265e-13, f1: 0.6647\n",
      "Progress 7/50, lambda_: 2.682695795279727e-13, f1: 0.6647\n",
      "Progress 8/50, lambda_: 5.963623316594636e-13, f1: 0.6647\n",
      "Progress 9/50, lambda_: 1.3257113655901109e-12, f1: 0.6647\n",
      "Progress 10/50, lambda_: 2.9470517025518096e-12, f1: 0.6647\n",
      "Progress 11/50, lambda_: 6.551285568595496e-12, f1: 0.6647\n",
      "Progress 12/50, lambda_: 1.4563484775012445e-11, f1: 0.6647\n",
      "Progress 13/50, lambda_: 3.237457542817653e-11, f1: 0.6647\n",
      "Progress 14/50, lambda_: 7.196856730011528e-11, f1: 0.6647\n",
      "Progress 15/50, lambda_: 1.5998587196060573e-10, f1: 0.6647\n",
      "Progress 16/50, lambda_: 3.5564803062231214e-10, f1: 0.6647\n",
      "Progress 17/50, lambda_: 7.906043210907701e-10, f1: 0.6647\n",
      "Progress 18/50, lambda_: 1.7575106248547966e-09, f1: 0.6647\n",
      "Progress 19/50, lambda_: 3.906939937054621e-09, f1: 0.6647\n",
      "Progress 20/50, lambda_: 8.68511373751352e-09, f1: 0.6647\n",
      "Progress 21/50, lambda_: 1.9306977288832496e-08, f1: 0.6647\n",
      "Progress 22/50, lambda_: 4.291934260128778e-08, f1: 0.6647\n",
      "Progress 23/50, lambda_: 9.540954763499944e-08, f1: 0.6647\n",
      "Progress 24/50, lambda_: 2.1209508879201927e-07, f1: 0.6647\n",
      "Progress 25/50, lambda_: 4.7148663634573897e-07, f1: 0.6647\n",
      "Progress 26/50, lambda_: 1.0481131341546875e-06, f1: 0.6647\n",
      "Progress 27/50, lambda_: 2.3299518105153717e-06, f1: 0.6647\n",
      "Progress 28/50, lambda_: 5.179474679231202e-06, f1: 0.6647\n",
      "Progress 29/50, lambda_: 1.1513953993264481e-05, f1: 0.6647\n",
      "Progress 30/50, lambda_: 2.559547922699533e-05, f1: 0.6647\n",
      "Progress 31/50, lambda_: 5.689866029018305e-05, f1: 0.6647\n",
      "Progress 32/50, lambda_: 0.00012648552168552957, f1: 0.6647\n",
      "Progress 33/50, lambda_: 0.0002811768697974225, f1: 0.6647\n",
      "Progress 34/50, lambda_: 0.0006250551925273976, f1: 0.6647\n",
      "Progress 35/50, lambda_: 0.001389495494373136, f1: 0.6647\n",
      "Progress 36/50, lambda_: 0.003088843596477485, f1: 0.6647\n",
      "Progress 37/50, lambda_: 0.006866488450042998, f1: 0.6645\n",
      "Progress 38/50, lambda_: 0.015264179671752302, f1: 0.6635\n",
      "Progress 39/50, lambda_: 0.0339322177189533, f1: 0.662\n",
      "Progress 40/50, lambda_: 0.07543120063354607, f1: 0.658\n",
      "Progress 41/50, lambda_: 0.167683293681101, f1: 0.6525\n",
      "Progress 42/50, lambda_: 0.3727593720314938, f1: 0.6454\n",
      "Progress 43/50, lambda_: 0.8286427728546826, f1: 0.6372\n",
      "Progress 44/50, lambda_: 1.8420699693267164, f1: 0.6261\n",
      "Progress 45/50, lambda_: 4.094915062380419, f1: 0.6119\n",
      "Progress 46/50, lambda_: 9.102981779915227, f1: 0.5987\n",
      "Progress 47/50, lambda_: 20.23589647725164, f1: 0.5907\n",
      "Progress 48/50, lambda_: 44.984326689694534, f1: 0.5868\n",
      "Progress 49/50, lambda_: 100.0, f1: 0.5851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda_': 1e-15, 'f1_score': 0.6647511072771948}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'ridge', max_iter=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/50, gamma :1e-06 f1: 0.44\n",
      "Progress 1/50, gamma :1.3102281887548672e-06 f1: 0.44\n",
      "Progress 2/50, gamma :1.7166979066078603e-06 f1: 0.44\n",
      "Progress 3/50, gamma :2.2492659888140894e-06 f1: 0.44\n",
      "Progress 4/50, gamma :2.94705170255181e-06 f1: 0.44\n",
      "Progress 5/50, gamma :3.861310214401406e-06 f1: 0.44\n",
      "Progress 6/50, gamma :5.059197488435822e-06 f1: 0.44\n",
      "Progress 7/50, gamma :6.628703161826441e-06 f1: 0.44\n",
      "Progress 8/50, gamma :8.68511373751352e-06 f1: 0.44\n",
      "Progress 9/50, gamma :1.1379480841432356e-05 f1: 0.44\n",
      "Progress 10/50, gamma :1.490971657184063e-05 f1: 0.44\n",
      "Progress 11/50, gamma :1.9535130938771177e-05 f1: 0.44\n",
      "Progress 12/50, gamma :2.559547922699533e-05 f1: 0.44\n",
      "Progress 13/50, gamma :3.353591838789893e-05 f1: 0.44\n",
      "Progress 14/50, gamma :4.393970560760795e-05 f1: 0.44\n",
      "Progress 15/50, gamma :5.757104089267825e-05 f1: 0.44\n",
      "Progress 16/50, gamma :7.543120063354622e-05 f1: 0.44\n",
      "Progress 17/50, gamma :9.883208538169628e-05 f1: 0.44\n",
      "Progress 18/50, gamma :0.0001294925842205263 f1: 0.44\n",
      "Progress 19/50, gamma :0.0001696648340804473 f1: 0.44\n",
      "Progress 20/50, gamma :0.00022229964825261955 f1: 0.4401\n",
      "Progress 21/50, gamma :0.00029126326549087385 f1: 0.4401\n",
      "Progress 22/50, gamma :0.0003816213407949357 f1: 0.4401\n",
      "Progress 23/50, gamma :0.0005000110381399525 f1: 0.4401\n",
      "Progress 24/50, gamma :0.0006551285568595509 f1: 0.4401\n",
      "Progress 25/50, gamma :0.0008583679024556795 f1: 0.4402\n",
      "Progress 26/50, gamma :0.0011246578221198195 f1: 0.4402\n",
      "Progress 27/50, gamma :0.0014735583812450464 f1: 0.4403\n",
      "Progress 28/50, gamma :0.0019306977288832516 f1: 0.4404\n",
      "Progress 29/50, gamma :0.0025296545883478384 f1: 0.4404\n",
      "Progress 30/50, gamma :0.003314424749466428 f1: 0.4406\n",
      "Progress 31/50, gamma :0.004342652736257703 f1: 0.4409\n",
      "Progress 32/50, gamma :0.005689866029018299 f1: 0.441\n",
      "Progress 33/50, gamma :0.007455022861458495 f1: 0.4413\n",
      "Progress 34/50, gamma :0.009767781100894893 f1: 0.4416\n",
      "Progress 35/50, gamma :0.012798022139979551 f1: 0.4421\n",
      "Progress 36/50, gamma :0.0167683293681101 f1: 0.443\n",
      "Progress 37/50, gamma :0.021970337816423945 f1: 0.444\n",
      "Progress 38/50, gamma :0.028786155923545713 f1: 0.4451\n",
      "Progress 39/50, gamma :0.03771643293692249 f1: 0.4466\n",
      "Progress 40/50, gamma :0.04941713361323838 f1: 0.4489\n",
      "Progress 41/50, gamma :0.06474772146753059 f1: 0.4519\n",
      "Progress 42/50, gamma :0.08483428982440726 f1: 0.456\n",
      "Progress 43/50, gamma :0.1111522779009386 f1: 0.4605\n",
      "Progress 44/50, gamma :0.14563484775012445 f1: 0.4672\n",
      "Progress 45/50, gamma :0.1908148827872364 f1: 0.4764\n",
      "Progress 46/50, gamma :0.25001103826179305 f1: 0.4882\n",
      "Progress 47/50, gamma :0.32757150983047295 f1: 0.5042\n",
      "Progress 48/50, gamma :0.42919342601287785 f1: 0.5265\n",
      "Progress 49/50, gamma :0.5623413251903491 f1: 0.5525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.5623413251903491, 'f1_score': 0.5524871711629684}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'logistic', max_iter=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/2500, lambda_: 1e-15,  gamma: 1e-06, f1: 0.44\n",
      "Progress 1/2500, lambda_: 2.222996482526191e-15,  gamma: 1e-06, f1: 0.44\n",
      "Progress 2/2500, lambda_: 4.9417133613238385e-15,  gamma: 1e-06, f1: 0.44\n",
      "Progress 3/2500, lambda_: 1.0985411419875573e-14,  gamma: 1e-06, f1: 0.44\n",
      "Progress 4/2500, lambda_: 2.4420530945486547e-14,  gamma: 1e-06, f1: 0.44\n",
      "Progress 5/2500, lambda_: 5.4286754393238596e-14,  gamma: 1e-06, f1: 0.44\n",
      "Progress 6/2500, lambda_: 1.2067926406393265e-13,  gamma: 1e-06, f1: 0.44\n",
      "Progress 7/2500, lambda_: 2.682695795279727e-13,  gamma: 1e-06, f1: 0.44\n",
      "Progress 8/2500, lambda_: 5.963623316594636e-13,  gamma: 1e-06, f1: 0.44\n",
      "Progress 9/2500, lambda_: 1.3257113655901109e-12,  gamma: 1e-06, f1: 0.44\n",
      "Progress 10/2500, lambda_: 2.9470517025518096e-12,  gamma: 1e-06, f1: 0.44\n",
      "Progress 11/2500, lambda_: 6.551285568595496e-12,  gamma: 1e-06, f1: 0.44\n",
      "Progress 12/2500, lambda_: 1.4563484775012445e-11,  gamma: 1e-06, f1: 0.44\n",
      "Progress 13/2500, lambda_: 3.237457542817653e-11,  gamma: 1e-06, f1: 0.44\n",
      "Progress 14/2500, lambda_: 7.196856730011528e-11,  gamma: 1e-06, f1: 0.44\n",
      "Progress 15/2500, lambda_: 1.5998587196060573e-10,  gamma: 1e-06, f1: 0.44\n",
      "Progress 16/2500, lambda_: 3.5564803062231214e-10,  gamma: 1e-06, f1: 0.44\n",
      "Progress 17/2500, lambda_: 7.906043210907701e-10,  gamma: 1e-06, f1: 0.44\n",
      "Progress 18/2500, lambda_: 1.7575106248547966e-09,  gamma: 1e-06, f1: 0.44\n",
      "Progress 19/2500, lambda_: 3.906939937054621e-09,  gamma: 1e-06, f1: 0.44\n",
      "Progress 20/2500, lambda_: 8.68511373751352e-09,  gamma: 1e-06, f1: 0.44\n",
      "Progress 21/2500, lambda_: 1.9306977288832496e-08,  gamma: 1e-06, f1: 0.44\n",
      "Progress 22/2500, lambda_: 4.291934260128778e-08,  gamma: 1e-06, f1: 0.44\n",
      "Progress 23/2500, lambda_: 9.540954763499944e-08,  gamma: 1e-06, f1: 0.44\n",
      "Progress 24/2500, lambda_: 2.1209508879201927e-07,  gamma: 1e-06, f1: 0.44\n",
      "Progress 25/2500, lambda_: 4.7148663634573897e-07,  gamma: 1e-06, f1: 0.44\n",
      "Progress 26/2500, lambda_: 1.0481131341546875e-06,  gamma: 1e-06, f1: 0.44\n",
      "Progress 27/2500, lambda_: 2.3299518105153717e-06,  gamma: 1e-06, f1: 0.44\n",
      "Progress 28/2500, lambda_: 5.179474679231202e-06,  gamma: 1e-06, f1: 0.44\n",
      "Progress 29/2500, lambda_: 1.1513953993264481e-05,  gamma: 1e-06, f1: 0.44\n",
      "Progress 30/2500, lambda_: 2.559547922699533e-05,  gamma: 1e-06, f1: 0.44\n",
      "Progress 31/2500, lambda_: 5.689866029018305e-05,  gamma: 1e-06, f1: 0.44\n",
      "Progress 32/2500, lambda_: 0.00012648552168552957,  gamma: 1e-06, f1: 0.44\n",
      "Progress 33/2500, lambda_: 0.0002811768697974225,  gamma: 1e-06, f1: 0.44\n",
      "Progress 34/2500, lambda_: 0.0006250551925273976,  gamma: 1e-06, f1: 0.44\n",
      "Progress 35/2500, lambda_: 0.001389495494373136,  gamma: 1e-06, f1: 0.44\n",
      "Progress 36/2500, lambda_: 0.003088843596477485,  gamma: 1e-06, f1: 0.44\n",
      "Progress 37/2500, lambda_: 0.006866488450042998,  gamma: 1e-06, f1: 0.44\n",
      "Progress 38/2500, lambda_: 0.015264179671752302,  gamma: 1e-06, f1: 0.44\n",
      "Progress 39/2500, lambda_: 0.0339322177189533,  gamma: 1e-06, f1: 0.44\n",
      "Progress 40/2500, lambda_: 0.07543120063354607,  gamma: 1e-06, f1: 0.44\n",
      "Progress 41/2500, lambda_: 0.167683293681101,  gamma: 1e-06, f1: 0.44\n",
      "Progress 42/2500, lambda_: 0.3727593720314938,  gamma: 1e-06, f1: 0.44\n",
      "Progress 43/2500, lambda_: 0.8286427728546826,  gamma: 1e-06, f1: 0.44\n",
      "Progress 44/2500, lambda_: 1.8420699693267164,  gamma: 1e-06, f1: 0.44\n",
      "Progress 45/2500, lambda_: 4.094915062380419,  gamma: 1e-06, f1: 0.44\n",
      "Progress 46/2500, lambda_: 9.102981779915227,  gamma: 1e-06, f1: 0.44\n",
      "Progress 47/2500, lambda_: 20.23589647725164,  gamma: 1e-06, f1: 0.44\n",
      "Progress 48/2500, lambda_: 44.984326689694534,  gamma: 1e-06, f1: 0.44\n",
      "Progress 49/2500, lambda_: 100.0,  gamma: 1e-06, f1: 0.44\n",
      "Progress 50/2500, lambda_: 1e-15,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 51/2500, lambda_: 2.222996482526191e-15,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 52/2500, lambda_: 4.9417133613238385e-15,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 53/2500, lambda_: 1.0985411419875573e-14,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 54/2500, lambda_: 2.4420530945486547e-14,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 55/2500, lambda_: 5.4286754393238596e-14,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 56/2500, lambda_: 1.2067926406393265e-13,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 57/2500, lambda_: 2.682695795279727e-13,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 58/2500, lambda_: 5.963623316594636e-13,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 59/2500, lambda_: 1.3257113655901109e-12,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 60/2500, lambda_: 2.9470517025518096e-12,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 61/2500, lambda_: 6.551285568595496e-12,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 62/2500, lambda_: 1.4563484775012445e-11,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 63/2500, lambda_: 3.237457542817653e-11,  gamma: 1.3102281887548672e-06, f1: 0.44\n",
      "Progress 64/2500, lambda_: 7.196856730011528e-11,  gamma: 1.3102281887548672e-06, f1: 0.44\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'reg_logistic', max_iter=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = least_squares(y_train, x_train_cleaned)\n",
    "y_pred = get_classification_pred(x_train_cleaned, weight)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = ridge_regression(y_train, x_train_cleaned, 10)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = mean_squared_error_gd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = mean_squared_error_sgd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = logistic_regression(y_train, x_train_cleaned, np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = reg_logistic_regression(y_train, x_train_cleaned, 0.2,  np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb6ad4b3c836f9389b495db494d9cfacf3d61f9be1f3aec1481f7e10dfc9325d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
