{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "from helper import load_csv_data\n",
    "from processing import *\n",
    "from implementations import *\n",
    "from feature_expansion import *\n",
    "from crossvalidation import *\n",
    "from metrics import f1_score, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "y_test , x_test , id_test  = load_csv_data(\"../data/test.csv\")\n",
    "# print('train data shape: ', x_train.shape, y_train.shape)\n",
    "# print('test  data shape: ', x_test.shape, y_test.shape)\n",
    "with open('../col_name.json', 'r') as file:\n",
    "    features = json.load(file)['col_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned = standardize(clean_data(x_train, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset depending on the Pri_Jet_number feature which can take values 0, 1, 2 or 3. Since the number values that are equal to 3 is really small, we will combine it with the values which have 2 so we will have a 3 subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "We will now do feature engineering to increase the results we will have. We do degree root transformation, polynomial transformation, logarithmic transformation and reciprocical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_finished = build_new_x(x_train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline\n",
    "\n",
    "cross validation pipeline. we will use 5-fold cross validation for choosing the optimal parameters.\n",
    "below is the list of models we will use throughout this process.\n",
    "\n",
    "1. least_squares (no parameter tuning needed.)\n",
    "2. ridge LS \n",
    "3. mse_gd\n",
    "4. mse_sgd\n",
    "5. logistic\n",
    "6. reg_logistic\n",
    "\n",
    "we will optimise the weigth based on the mse loss. But the selection process will be based on observing F1 score on validation set.\n",
    "\n",
    "To see the effect of data manipulation, we will try 3 sets of data.\n",
    "1. cleaned data\n",
    "2. standardized data\n",
    "3. feature-engineered data\n",
    "\n",
    "From this process, we would expect the 3rd trial would give us the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "class HyperParameterTuner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        model_name: str,\n",
    "        num_folds: int=5,\n",
    "        num_seed: int=0,\n",
    "        max_iter: int=10,\n",
    "        grid_size: int=10\n",
    "    ):\n",
    "\n",
    "        available_models = {\n",
    "            'least_squares': least_squares,\n",
    "            'ridge': ridge_regression,\n",
    "            'mse_gd': mean_squared_error_gd,\n",
    "            'mse_sgd': mean_squared_error_sgd,\n",
    "            'logistic': logistic_regression,\n",
    "            'reg_logistic': reg_logistic_regression\n",
    "        }\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = available_models[model_name]\n",
    "        self.model_name = model_name\n",
    "        self.num_folds = num_folds\n",
    "        self.num_seed  = num_seed\n",
    "        self.max_iter  = max_iter\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # build k_indices\n",
    "        np.random.seed(self.num_seed)\n",
    "        self.build_k_indices()\n",
    "\n",
    "        # get model params given model specs.\n",
    "        model_parameters = {\n",
    "            'least_squares': {}, \n",
    "            'ridge'        : {'lambda_': None},\n",
    "            'mse_gd'       : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'mse_sgd'      : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'logistic'     : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'reg_logistic' : {'initial_w': np.ones((x.shape[1],)), 'max_iters': self.max_iter, 'gamma': None, 'lambda_': None},\n",
    "        }\n",
    "        self.hyp_params = model_parameters[model_name]\n",
    "\n",
    "    def tune_(self) -> Tuple[list, float]:\n",
    "        \"\"\"\n",
    "        hyperparameter tuning done by grid search.\n",
    "        best parameters are found by finding the maximum f1 scores.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas = np.logspace(-15,    2, self.grid_size)\n",
    "        gammas  = np.logspace(-6 ,-0.25, self.grid_size)     \n",
    "        \n",
    "        f1_scores = np.array([])\n",
    "        params = self.hyp_params\n",
    "        \n",
    "        if self.model_name == 'least_squares':\n",
    "            results = np.array([[k, self.cross_validation_per_k(k, self.hyp_params)[-1]] for k in range(self.num_folds)])\n",
    "            return results[np.argmax(results[:,-1])]   \n",
    "        \n",
    "        elif self.model_name == 'reg_logistic':\n",
    "            lambda_and_gammas = list(product(gammas, lambdas))\n",
    "            for i, (gamma, lambda_) in enumerate(lambda_and_gammas):\n",
    "                params['gamma'], params['lambda_'] = gamma, lambda_\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                if i %10 == 0:\n",
    "                    print(f'Progress {i}/{self.grid_size**2}, lambda_: {lambda_},  gamma: {gamma}, f1: {np.round(np.mean(results, axis=0)[-1], 4)}')            \n",
    "                       \n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            print(optimum_idx, len(f1_scores), len(list(lambda_and_gammas)))\n",
    "            best_params, best_f1 = lambda_and_gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'params' :best_params, 'f1_score' :best_f1}     \n",
    "\n",
    "        elif self.model_name == 'ridge':\n",
    "            for i, lambda_ in enumerate(lambdas):\n",
    "                params['lambda_'] =lambda_\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                if i %10 == 0:\n",
    "                    print(f'Progress {i}/{len(lambdas)}, lambda_: {lambda_}, f1: {np.round(np.mean(results, axis=0)[-1], 4)}')            \n",
    "            \n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = lambdas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'lambda_' :best_params, 'f1_score' :best_f1}     \n",
    "        \n",
    "        else:\n",
    "            for i, gamma in enumerate(gammas):\n",
    "                params['gamma'] = gamma\n",
    "                results = np.array([self.cross_validation_per_k(k, params) for k in range(self.num_folds)])\n",
    "                f1_scores = np.append(f1_scores, np.mean(results, axis=0)[-1])\n",
    "                if i %10 == 0:\n",
    "                    print(f'Progress {i}/{len(gammas)}, gamma :{gamma} f1: {np.round(np.mean(results, axis=0)[-1], 4)}')\n",
    "            # set nan values to 0.\n",
    "            f1_scores[np.isnan(f1_scores)] = 0 \n",
    "            \n",
    "            # get optimum index\n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return {'gamma' :best_params, 'f1_score' :best_f1}\n",
    "        \n",
    "    def cross_validation_per_k(self, k: int, params: dict) -> list:\n",
    "        \"\"\"return the loss of given model.\"\"\"\n",
    "        # get k'th subgroup in test, others in train\n",
    "        tr_indices, te_indices = self.k_indices[~(np.arange(self.k_indices.shape[0]) == k)].reshape(-1),\\\n",
    "                                 self.k_indices[k]\n",
    "        \n",
    "        # split the data based on train and validation indices\n",
    "        y_trn, y_val = self.y[tr_indices], self.y[te_indices]\n",
    "        x_trn, x_val = self.x[tr_indices], self.x[te_indices]\n",
    "\n",
    "        # run the model\n",
    "        params['tx'], params['y'] = x_trn, y_trn\n",
    "        w, _ = self.model(**params)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        loss_trn = np.sqrt(mse_loss(y_trn, x_trn, w))\n",
    "        loss_val = np.sqrt(mse_loss(y_val, x_val, w))\n",
    "        \n",
    "        # get validation f1-score\n",
    "        y_pred = get_classification_pred(x_val, w)\n",
    "        f1_val = f1_score(y_val, y_pred)\n",
    "        return [loss_trn, loss_val, f1_val]\n",
    "\n",
    "    def build_k_indices(self):\n",
    "        \"\"\"\n",
    "        build k indices for k-fold.\n",
    "        Args:\n",
    "            y:      shape=(N,)\n",
    "            k_fold: K in K-fold, i.e. the fold num\n",
    "            seed:   the random seed\n",
    "\n",
    "        Returns:\n",
    "            A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "        \"\"\"\n",
    "        num_row  = self.y.shape[0]\n",
    "        interval = int(num_row / self.num_folds)\n",
    "        indices  = np.random.permutation(num_row)\n",
    "\n",
    "        self.k_indices = np.array([indices[k * interval : (k + 1) * interval] for k in range(self.num_folds)])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.66636489])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'least_squares', max_iter=100, grid_size=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/20, gamma :1e-06 f1: 0.44\n",
      "Progress 10/20, gamma :0.001062467830894041 f1: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:135: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/metrics.py:30: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precission = true_positive / (true_positive + false_positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.280135676119887, 'f1_score': 0.6626473443870915}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'mse_gd', max_iter=50, grid_size=20)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/20, gamma :1e-06 f1: 0.44\n",
      "Progress 10/20, gamma :0.001062467830894041 f1: 0.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/implementations.py:135: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(t) / (1 + np.exp(t))\n",
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/metrics.py:30: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precission = true_positive / (true_positive + false_positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 2.007389180054629e-06, 'f1_score': 0.43999712896575083}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'mse_sgd', max_iter=50, grid_size=20)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/25, lambda_: 1e-15, f1: 0.6648\n",
      "Progress 10/25, lambda_: 1.2115276586285901e-08, f1: 0.6647\n",
      "Progress 20/25, lambda_: 0.14677992676220736, f1: 0.6537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda_': 1e-15, 'f1_score': 0.6647511072771948}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'ridge', max_iter=50, grid_size=25)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/20, gamma :1e-06 f1: 0.44\n",
      "Progress 10/20, gamma :0.001062467830894041 f1: 0.441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.5623413251903491, 'f1_score': 0.6381401682827477}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'logistic', max_iter=50, grid_size=20)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0/100, lambda_: 1e-15,  gamma: 1e-06, f1: 0.44\n",
      "Progress 10/100, lambda_: 1e-15,  gamma: 4.354004653656653e-06, f1: 0.44\n",
      "Progress 20/100, lambda_: 1e-15,  gamma: 1.8957356524063752e-05, f1: 0.44\n",
      "Progress 30/100, lambda_: 1e-15,  gamma: 8.254041852680174e-05, f1: 0.44\n",
      "Progress 40/100, lambda_: 1e-15,  gamma: 0.00035938136638046257, f1: 0.4401\n",
      "Progress 50/100, lambda_: 1e-15,  gamma: 0.001564748141658019, f1: 0.4403\n",
      "Progress 60/100, lambda_: 1e-15,  gamma: 0.006812920690579608, f1: 0.4412\n",
      "Progress 70/100, lambda_: 1e-15,  gamma: 0.02966348839177725, f1: 0.4453\n",
      "Progress 80/100, lambda_: 1e-15,  gamma: 0.12915496650148828, f1: 0.4638\n",
      "Progress 90/100, lambda_: 1e-15,  gamma: 0.5623413251903491, f1: 0.5525\n",
      "88 100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': (0.12915496650148828, 1.2915496650148828),\n",
       " 'f1_score': 0.5947337865658154}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = HyperParameterTuner(x_train_cleaned, y_train, 'reg_logistic', max_iter=10, grid_size=10)\n",
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = least_squares(y_train, x_train_cleaned)\n",
    "y_pred = get_classification_pred(x_train_cleaned, weight)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = ridge_regression(y_train, x_train_cleaned, 10)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = mean_squared_error_gd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, loss = mean_squared_error_sgd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = logistic_regression(y_train, x_train_cleaned, np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = reg_logistic_regression(y_train, x_train_cleaned, 0.2,  np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5623413251903491, 100.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = np.logspace(-15,    2, 10)\n",
    "gammas  = np.logspace(-6 ,-0.25, 10)     \n",
    "lambda_and_gammas = product(gammas, lambdas)\n",
    "a = list(lambda_and_gammas)[99]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb6ad4b3c836f9389b495db494d9cfacf3d61f9be1f3aec1481f7e10dfc9325d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
