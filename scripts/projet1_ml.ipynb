{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import *\n",
    "from processing import *\n",
    "from implementations import *\n",
    "from feature_expansion import *\n",
    "from crossvalidation import *\n",
    "from metrics import f1_score, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "y_test , x_test , id_test  = load_csv_data(\"../data/test.csv\")\n",
    "# print('train data shape: ', x_train.shape, y_train.shape)\n",
    "# print('test  data shape: ', x_test.shape, y_test.shape)\n",
    "with open('../col_name.json', 'r') as file:\n",
    "    features = json.load(file)['col_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned = standardize(clean_data(x_train, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset depending on the Pri_Jet_number feature which can take values 0, 1, 2 or 3. Since the number values that are equal to 3 is really small, we will combine it with the values which have 2 so we will have a 3 subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "We will now do feature engineering to increase the results we will have. We do degree root transformation, polynomial transformation, logarithmic transformation and reciprocical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/PycharmProjects/MLprojec1/scripts/feature_expansion.py:21: RuntimeWarning: invalid value encountered in log\n",
      "  log_column = np.log(1 + log_column)\n"
     ]
    }
   ],
   "source": [
    "x_train_finished = build_new_x(x_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724309\n"
     ]
    }
   ],
   "source": [
    "test_data = clean_data(x_train, features)\n",
    "print(np.isnan(x_train_finished).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline\n",
    "\n",
    "cross validation pipeline. we will use 5-fold cross validation for choosing the optimal parameters.\n",
    "below is the list of models we will use throughout this process.\n",
    "\n",
    "1. least_squares (no parameter tuning needed.)\n",
    "2. ridge LS \n",
    "3. mse_gd\n",
    "4. mse_sgd\n",
    "5. logistic\n",
    "6. reg_logistic\n",
    "\n",
    "we will optimise the weigth based on the mse loss. But the selection process will be based on observing F1 score on validation set.\n",
    "\n",
    "To see the effect of data manipulation, we will try 3 sets of data.\n",
    "1. cleaned data\n",
    "2. standardized data\n",
    "3. feature-engineered data\n",
    "\n",
    "From this process, we would expect the 3rd trial would give us the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indices = k_indices[k]\n",
    "    tr_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    \n",
    "    # split the data based on train and validation indices\n",
    "    y_trn, y_val = y[tr_indices], y[te_indices]\n",
    "    x_trn, x_val = x[tr_indices], x[te_indices]\n",
    "\n",
    "    # ridge regression\n",
    "    w, _ = ridge_regression(y_trn, x_trn, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data\n",
    "    loss_trn = np.sqrt(mse_loss(y_trn, x_trn, w))\n",
    "    loss_val = np.sqrt(mse_loss(y_val, x_val, w))\n",
    "    \n",
    "    # get validation f1-score\n",
    "    y_pred = get_classification_pred(x_val, w)\n",
    "    f1_val = f1_score(y_val, y_pred)\n",
    "    return loss_trn, loss_val, f1_val\n",
    "\n",
    "def best_degree_selection(y, x, k_fold, seed = 1):\n",
    "    # define the range of lambda values to try.\n",
    "    lambdas = np.logspace(-15,0,100)\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # cross validation\n",
    "    f1_scores = []\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_te_tmp = []\n",
    "        results = [cross_validation(y, x, k_indices, k, lambda_) for k in range(k_fold)]\n",
    "        f1_scores.append(np.mean(results, axis=0)[-1])\n",
    "    \n",
    "    optimum_idx = np.argmax(f1_scores)\n",
    "    best_lambda, best_f1 = lambdas[optimum_idx], f1_scores[optimum_idx]\n",
    "    print(f\" Lambda : {best_lambda}, F1: {best_f1}\")\n",
    "    return best_lambda, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lambda : 0.043287612810830614, F1: 0.5688146561392932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.043287612810830614, 0.5688146561392932)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_degree_selection(y_train, x_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51004\n"
     ]
    }
   ],
   "source": [
    "weight, loss = least_squares(y_train, x_train_cleaned)\n",
    "print(compute_mse_loss(y_train, x_train_cleaned, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509928\n"
     ]
    }
   ],
   "source": [
    "weight, loss = ridge_regression(y_train, x_train_cleaned, 10)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.215536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, loss = mean_squared_error_gd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, loss = mean_squared_error_sgd(y_train, x_train_cleaned, np.ones((31,)), 100, 1e-3)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.995336631273058\n",
      "Current iteration=100, loss=1.0491363661898327\n",
      "Current iteration=200, loss=0.27006069549426187\n",
      "Current iteration=300, loss=-0.37199300383762574\n",
      "Current iteration=400, loss=-0.8974960829542113\n",
      "Current iteration=500, loss=-1.3222079469218488\n",
      "Current iteration=600, loss=-1.661546133055785\n",
      "Current iteration=700, loss=-1.9325063200387396\n",
      "Current iteration=800, loss=-2.15229341612981\n",
      "Current iteration=900, loss=-2.335662630699634\n"
     ]
    }
   ],
   "source": [
    "w, l = logistic_regression(y_train, x_train_cleaned, np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.995336631273058\n",
      "Current iteration=100, loss=0.9989272803604264\n",
      "Current iteration=200, loss=0.20757768961068648\n",
      "Current iteration=300, loss=-0.41169620130994156\n",
      "Current iteration=400, loss=-0.8829184065559729\n",
      "Current iteration=500, loss=-1.2271743752375226\n",
      "Current iteration=600, loss=-1.4682340429038723\n",
      "Current iteration=700, loss=-1.632840431601386\n",
      "Current iteration=800, loss=-1.745383871074444\n",
      "Current iteration=900, loss=-1.8236927657875832\n"
     ]
    }
   ],
   "source": [
    "w, l = reg_logistic_regression(y_train, x_train_cleaned, 0.2,  np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb6ad4b3c836f9389b495db494d9cfacf3d61f9be1f3aec1481f7e10dfc9325d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
