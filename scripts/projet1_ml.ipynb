{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from implementations import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import load_csv_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = np.genfromtxt(\"../data/train.csv\",\n",
    "              delimiter=',',\n",
    "              encoding='UTF-8-sig',\n",
    "              dtype=None,\n",
    "              names=True).dtype.names[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30) (568238,)\n"
     ]
    }
   ],
   "source": [
    "y_test, x_test, id_test = load_csv_data(\"../data/test.csv\")\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84921.70243584755\n"
     ]
    }
   ],
   "source": [
    "weight, loss = least_squares(y_train, x_train)\n",
    "print(compute_mse_loss(y_train, x_train, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=6314061601368.943, w0=-4209582884.6052313, w1=-4209582884.6052313\n",
      "GD iter. 1/99: loss=1.1189997026737242e+32, w0=1.7721466046753657e+19, w1=1.7721466046753657e+19\n",
      "GD iter. 2/99: loss=1.9831297750210564e+51, w0=-7.460367629171619e+28, w1=-7.460367629171619e+28\n",
      "GD iter. 3/99: loss=3.514570821759856e+70, w0=3.140659187877261e+38, w1=3.140659187877261e+38\n",
      "GD iter. 4/99: loss=6.228643337793979e+89, w0=-1.3221520204753111e+48, w1=-1.3221520204753111e+48\n",
      "GD iter. 5/99: loss=1.1038616035063706e+109, w0=5.565984274875964e+57, w1=5.565984274875964e+57\n",
      "GD iter. 6/99: loss=1.956301514813062e+128, w0=-2.3431633025852177e+67, w1=-2.3431633025852177e+67\n",
      "GD iter. 7/99: loss=3.46702485592687e+147, w0=9.864228843342201e+76, w1=9.864228843342201e+76\n",
      "GD iter. 8/99: loss=6.144380741208666e+166, w0=-4.152634627149947e+86, w1=-4.152634627149947e+86\n",
      "GD iter. 9/99: loss=1.0889282962132917e+186, w0=1.7481725759275883e+96, w1=1.7481725759275883e+96\n",
      "GD iter. 10/99: loss=1.929836193811007e+205, w0=-7.359441967864197e+105, w1=-7.359441967864197e+105\n",
      "GD iter. 11/99: loss=3.420122103442496e+224, w0=3.098171589244932e+115, w1=3.098171589244932e+115\n",
      "GD iter. 12/99: loss=6.0612580694509614e+243, w0=-1.3042656275187953e+125, w1=-1.3042656275187953e+125\n",
      "GD iter. 13/99: loss=1.0741970103203573e+263, w0=5.490686290689227e+134, w1=5.490686290689227e+134\n",
      "GD iter. 14/99: loss=1.9037289020853733e+282, w0=-2.3114644215622585e+144, w1=-2.3114644215622585e+144\n",
      "GD iter. 15/99: loss=3.373853862760573e+301, w0=9.730783164953736e+153, w1=9.730783164953736e+153\n",
      "GD iter. 16/99: loss=inf, w0=-4.096456779522937e+163, w1=-4.096456779522937e+163\n",
      "GD iter. 17/99: loss=inf, w0=1.724522873650847e+173, w1=1.724522873650847e+173\n",
      "GD iter. 18/99: loss=inf, w0=-7.259881653362204e+182, w1=-7.259881653362204e+182\n",
      "GD iter. 19/99: loss=inf, w0=3.0562587731437924e+192, w1=3.0562587731437924e+192\n",
      "GD iter. 20/99: loss=inf, w0=-1.2866212060209792e+202, w1=-1.2866212060209792e+202\n",
      "GD iter. 21/99: loss=inf, w0=5.416406955881075e+211, w1=5.416406955881075e+211\n",
      "GD iter. 22/99: loss=inf, w0=-2.280194370683975e+221, w1=-2.280194370683975e+221\n",
      "GD iter. 23/99: loss=inf, w0=9.599142772043089e+230, w1=9.599142772043089e+230\n",
      "GD iter. 24/99: loss=inf, w0=-4.041038919433272e+240, w1=-4.041038919433272e+240\n",
      "GD iter. 25/99: loss=inf, w0=1.7011931102779888e+250, w1=1.7011931102779888e+250\n",
      "GD iter. 26/99: loss=inf, w0=-7.161668214923249e+259, w1=-7.161668214923249e+259\n",
      "GD iter. 27/99: loss=inf, w0=3.014912963776382e+269, w1=3.014912963776382e+269\n",
      "GD iter. 28/99: loss=inf, w0=-1.2692154825332605e+279, w1=-1.2692154825332605e+279\n",
      "GD iter. 29/99: loss=inf, w0=5.343132489915656e+288, w1=5.343132489915656e+288\n",
      "GD iter. 30/99: loss=inf, w0=-2.2493473486322475e+298, w1=-2.2493473486322475e+298\n",
      "GD iter. 31/99: loss=inf, w0=nan, w1=nan\n",
      "GD iter. 32/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 33/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 34/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 35/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 36/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 37/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 38/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 39/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 40/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 41/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 42/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 43/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 44/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 45/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 46/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 47/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 48/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 49/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 50/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 51/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 52/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 53/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 54/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 55/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 56/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 57/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 58/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 59/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 60/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 61/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 62/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 63/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 64/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 65/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 66/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 67/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 68/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 69/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 70/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 71/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 72/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 73/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 74/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 75/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 76/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 77/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 78/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 79/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 80/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 81/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 82/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 83/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 84/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 85/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 86/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 87/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 88/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 89/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 90/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 91/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 92/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 93/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 94/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 95/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 96/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 97/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 98/99: loss=nan, w0=nan, w1=nan\n",
      "GD iter. 99/99: loss=nan, w0=nan, w1=nan\n"
     ]
    }
   ],
   "source": [
    "weight, loss = mean_squared_error_gd(y_train, x_train, np.ones((30,)), 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84921.70241460518\n"
     ]
    }
   ],
   "source": [
    "weight, loss = ridge_regression(y_train, x_train, 0.025)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 466. GiB for an array with shape (250000, 250000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4894/2579013891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Master/ml/MLprojec1/scripts/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_GD\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         loss, gradient = calculate_nll_loss(y, tx, w), calculate_logistic_gradient(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/Master/ml/MLprojec1/scripts/implementations.py\u001b[0m in \u001b[0;36mcalculate_nll_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 466. GiB for an array with shape (250000, 250000) and data type float64"
     ]
    }
   ],
   "source": [
    "weights, loss = logistic_regression_GD(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
