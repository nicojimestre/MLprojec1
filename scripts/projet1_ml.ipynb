{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "from helper import load_csv_data\n",
    "from processing import *\n",
    "from implementations import *\n",
    "from feature_expansion import *\n",
    "from crossvalidation import *\n",
    "from metrics import f1_score, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the data\n",
    "Import of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data(\"../data/train.csv\")\n",
    "y_test , x_test , id_test  = load_csv_data(\"../data/test.csv\")\n",
    "# print('train data shape: ', x_train.shape, y_train.shape)\n",
    "# print('test  data shape: ', x_test.shape, y_test.shape)\n",
    "with open('../col_name.json', 'r') as file:\n",
    "    features = json.load(file)['col_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "We pre process the data to get a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned = standardize(clean_data(x_train, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.genfromtxt(\"../data/train.csv\",\n",
    "              delimiter=',',\n",
    "              encoding='UTF-8-sig',\n",
    "              dtype=None,\n",
    "              names=True).dtype.names[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset depending on the Pri_Jet_number feature which can take values 0, 1, 2 or 3. Since the number values that are equal to 3 is really small, we will combine it with the values which have 2 so we will have a 3 subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature expansion\n",
    "We will now do feature engineering to increase the results we will have. We do degree root transformation, polynomial transformation, logarithmic transformation and reciprocical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eoeoeoeoeoeoeoeoe!!!!!!!!!!!!!!!!!!!!!\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x_train_finished = build_new_x(x_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_0, y_0, X_1, y_1, X_23, y_23 = pre_process_data(x_train, y_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_0:\n",
    "    if(all([b.is_integer() for b in i] or all([b.is_float() for b in i]) == False):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Pipeline\n",
    "\n",
    "cross validation pipeline. we will use 5-fold cross validation for choosing the optimal parameters.\n",
    "below is the list of models we will use throughout this process.\n",
    "\n",
    "1. least_squares (no parameter tuning needed.)\n",
    "2. ridge LS \n",
    "3. mse_gd\n",
    "4. mse_sgd\n",
    "5. logistic\n",
    "6. reg_logistic\n",
    "\n",
    "we will optimise the weigth based on the mse loss. But the selection process will be based on observing F1 score on validation set.\n",
    "\n",
    "To see the effect of data manipulation, we will try 3 sets of data.\n",
    "1. cleaned data\n",
    "2. standardized data\n",
    "3. feature-engineered data\n",
    "\n",
    "From this process, we would expect the 3rd trial would give us the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(d):\n",
    "      \n",
    "    for key in d:\n",
    "        print(\"key:\", key, \"Value:\", d[key])\n",
    "          \n",
    "# Driver's code\n",
    "D = {'':1, 'b':2, 'c':3}\n",
    "func(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.03299588e-05, -7.20237523e-03, -6.05398451e-03, -5.47555763e-04,\n",
       "        -1.93895382e-02,  4.73455547e-04, -2.60381502e-02,  3.25107737e-01,\n",
       "        -3.80965362e-05, -2.72728860e+00, -2.21218488e-01,  9.50801264e-02,\n",
       "         6.40374762e-02,  2.73554833e+00, -3.31802422e-04, -9.54328018e-04,\n",
       "         2.74030502e+00, -5.34164915e-04,  9.73498603e-04,  3.69225052e-03,\n",
       "         3.54487433e-04, -5.43344599e-04, -3.30448035e-01, -1.40800498e-03,\n",
       "         8.31432880e-04,  1.02117272e-03, -1.68047416e-03, -5.83664815e-03,\n",
       "        -1.11087998e-02,  2.72774855e+00]),\n",
       " 0.33968680955669167)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_dict = {'tx': x_train, 'y': y_train}\n",
    "least_squares(**ls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "class HyperParameterTuner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        model_name: str,\n",
    "        num_folds: int,\n",
    "        num_seed: int=0,\n",
    "        max_iter: int=1000  \n",
    "    ):\n",
    "\n",
    "        available_models = {\n",
    "        'least_squares': least_squares,\n",
    "        'ridge': ridge_regression,\n",
    "        'mse_gd': mean_squared_error_gd,\n",
    "        'mse_sgd': mean_squared_error_sgd,\n",
    "        'logistic': logistic_regression,\n",
    "        'reg_logistic': reg_logistic_regression\n",
    "        }\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = available_models[model_name]\n",
    "        self.model_name = model_name\n",
    "        self.num_folds = num_folds\n",
    "        self.num_seed  = num_seed\n",
    "        self.max_iter  = max_iter\n",
    "        \n",
    "        # build k_indices\n",
    "        np.random.seed(self.num_seed)\n",
    "        self.build_k_indices()\n",
    "\n",
    "        # get model params given model specs.\n",
    "        model_parameters = {\n",
    "            'least_squares': {}, \n",
    "            'ridge'        : {'lambda_': None},\n",
    "            'mse_gd'       : {'initial_w': np.ones((len(y),1)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'mse_sgd'      : {'initial_w': np.ones((len(y),1)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'logistic'     : {'initial_w': np.ones((len(y),1)), 'max_iters': self.max_iter, 'gamma': None},\n",
    "            'reg_logistic' : {'initial_w': np.ones((len(y),1)), 'max_iters': self.max_iter, 'gamma': None, 'lambda_': None},\n",
    "        }\n",
    "        self.hyp_params = model_parameters[model_name]\n",
    "\n",
    "    def tune_(self) -> Tuple[list, float]:\n",
    "        \"\"\"\n",
    "        hyperparameter tuning done by grid search.\n",
    "        best parameters are found by finding the maximum f1 scores.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas = np.logspace(-15,0,100)\n",
    "        gammas  = np.linspace(0,1,100)        \n",
    "\n",
    "        f1_scores = []\n",
    "        params = self.hyp_params\n",
    "        # cross validation\n",
    "        if self.model_name == 'least_squares':\n",
    "            results = np.array([[k, self.cross_validation_per_k(k, self.hyp_params)[-1]] for k in range(self.num_folds)])\n",
    "            return results[np.argmax(results[:,-1])]   \n",
    "        \n",
    "        elif self.model_name == 'reg_logistic':\n",
    "            lambda_and_gammas = product(gammas, lambdas)\n",
    "            for (gamma, lambda_) in lambda_and_gammas:\n",
    "                params['gamma'], params['lambda_'] = gamma, lambda_\n",
    "                results = np.concatenate([self.cross_validation_per_k(k, params) for k in range(self.num_folds)], axis=0)\n",
    "                f1_scores.append(np.mean(results, axis=0)[-1])\n",
    "        \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = lambda_and_gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return best_params, best_f1     \n",
    "\n",
    "        elif self.model_name == 'ridge':\n",
    "            for lambda_ in lambdas:\n",
    "                params['lambda_'] =lambda_\n",
    "                results = np.concatenate([self.cross_validation_per_k(k, params) for k in range(self.num_folds)], axis=0)\n",
    "                f1_scores.append(np.mean(results, axis=0)[-1])\n",
    "                        \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = lambdas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return best_params, best_f1     \n",
    "        else:\n",
    "            for gamma in gammas:\n",
    "                params['gamma'] = gamma\n",
    "                results = np.concatenate([self.cross_validation_per_k(k, params) for k in range(self.num_folds)], axis=0)\n",
    "                f1_scores.append(np.mean(results, axis=0)[-1])\n",
    "            \n",
    "            optimum_idx = np.argmax(f1_scores)\n",
    "            best_params, best_f1 = gammas[optimum_idx], f1_scores[optimum_idx]\n",
    "            return best_params, best_f1     \n",
    "        \n",
    "\n",
    "    def cross_validation_per_k(self, k: int, params: dict):\n",
    "        \"\"\"return the loss of given model.\"\"\"\n",
    "        # get k'th subgroup in test, others in train\n",
    "        tr_indices, te_indices = self.k_indices[~(np.arange(self.k_indices.shape[0]) == k)].reshape(-1),\\\n",
    "                                 self.k_indices[k]\n",
    "        \n",
    "        # split the data based on train and validation indices\n",
    "        y_trn, y_val = self.y[tr_indices], self.y[te_indices]\n",
    "        x_trn, x_val = self.x[tr_indices], self.x[te_indices]\n",
    "\n",
    "        # run the model\n",
    "        params['tx'], params['y'] = x_trn, y_trn\n",
    "        w, _ = self.model(**params)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        loss_trn = np.sqrt(mse_loss(y_trn, x_trn, w))\n",
    "        loss_val = np.sqrt(mse_loss(y_val, x_val, w))\n",
    "        \n",
    "        # get validation f1-score\n",
    "        y_pred = get_classification_pred(x_val, w)\n",
    "        f1_val = f1_score(y_val, y_pred)\n",
    "        return loss_trn, loss_val, f1_val\n",
    "\n",
    "    def build_k_indices(self):\n",
    "        \"\"\"\n",
    "        build k indices for k-fold.\n",
    "        Args:\n",
    "            y:      shape=(N,)\n",
    "            k_fold: K in K-fold, i.e. the fold num\n",
    "            seed:   the random seed\n",
    "\n",
    "        Returns:\n",
    "            A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "        \"\"\"\n",
    "        num_row  = self.y.shape[0]\n",
    "        interval = int(num_row / self.num_folds)\n",
    "        indices  = np.random.permutation(num_row)\n",
    "\n",
    "        self.k_indices = np.array([indices[k * interval : (k + 1) * interval] for k in range(self.num_folds)])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperParameterTuner(x_train, y_train, 'least_squares', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.        , 0.57177354])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n",
      "(77544, 23)\n",
      "(72543, 30)\n",
      "0.5592677345537758\n",
      "0.530324932436679\n",
      "0.6845379481959054\n"
     ]
    }
   ],
   "source": [
    "w_0, loss_0 = least_squares(y_0, X_0)\n",
    "w_1, loss_1 = least_squares(y_1, X_1)\n",
    "w_23, loss_23 = least_squares(y_23, X_23)\n",
    "y_pred_0 = get_classification_pred(X_0, w_0)\n",
    "y_pred_1 = get_classification_pred(X_1, w_1)\n",
    "y_pred_23 = get_classification_pred(X_23, w_23)\n",
    "print(f1_score(y_0, y_pred_0))\n",
    "print(f1_score(y_1, y_pred_1))\n",
    "print(f1_score(y_23, y_pred_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with ridges regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6259504278027698\n",
      "0.5559456972792309\n",
      "0.6526675851584398\n"
     ]
    }
   ],
   "source": [
    "w_0, loss_0 = ridge_regression(y_0, X_0, 10)\n",
    "w_1, loss_1 = ridge_regression(y_1, X_1, 10)\n",
    "w_23, loss_23 = ridge_regression(y_23, X_23, 10)\n",
    "y_pred_0 = get_classification_pred(X_0, w_0)\n",
    "y_pred_1 = get_classification_pred(X_1, w_1)\n",
    "y_pred_23 = get_classification_pred(X_23, w_23)\n",
    "print(f1_score(y_0, y_pred_0))\n",
    "print(f1_score(y_1, y_pred_1))\n",
    "print(f1_score(y_23, y_pred_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3424220782533286\n",
      "0.4789386303865536\n",
      "0.5330964394854666\n"
     ]
    }
   ],
   "source": [
    "w_0, loss_0 = mean_squared_error_gd(y_0, X_0, np.ones((19,)), 100, 1e-3)\n",
    "w_1, loss_1 = mean_squared_error_gd(y_1, X_1, np.ones((23,)), 100, 1e-3)\n",
    "w_23, loss_23 = mean_squared_error_gd(y_23, X_23, np.ones((30,)), 100, 1e-3)\n",
    "y_pred_0 = get_classification_pred(X_0, w_0)\n",
    "y_pred_1 = get_classification_pred(X_1, w_1)\n",
    "y_pred_23 = get_classification_pred(X_23, w_23)\n",
    "print(f1_score(y_0, y_pred_0))\n",
    "print(f1_score(y_1, y_pred_1))\n",
    "print(f1_score(y_23, y_pred_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (99913,) (99913,19) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11671/973110487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw_23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_23\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classification_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classification_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master/ml/MLprojec1/scripts/implementations.py\u001b[0m in \u001b[0;36mmean_squared_error_sgd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master/ml/MLprojec1/scripts/metrics.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcalculates\u001b[0m \u001b[0mmse\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (99913,) (99913,19) "
     ]
    }
   ],
   "source": [
    "w_0, loss_0 = mean_squared_error_sgd(y_0, X_0, np.ones((19,)), 100, 1e-3)\n",
    "w_1, loss_1 = mean_squared_error_sgd(y_1, X_1, np.ones((23,)), 100, 1e-3)\n",
    "w_23, loss_23 = mean_squared_error_sgd(y_23, X_23, np.ones((30,)), 100, 1e-3)\n",
    "y_pred_0 = get_classification_pred(X_0, w_0)\n",
    "y_pred_1 = get_classification_pred(X_1, w_1)\n",
    "y_pred_23 = get_classification_pred(X_23, w_23)\n",
    "print(f1_score(y_0, y_pred_0))\n",
    "print(f1_score(y_1, y_pred_1))\n",
    "print(f1_score(y_23, y_pred_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = logistic_regression(y_train, x_train_cleaned, np.ones((31,)), 1000, 1e-3)\n",
    "w_0, loss_0 = mean_squared_error_sgd(y_0, X_0, np.ones((19,)), 100, 1e-3)\n",
    "w_1, loss_1 = mean_squared_error_sgd(y_1, X_1, np.ones((23,)), 100, 1e-3)\n",
    "w_23, loss_23 = mean_squared_error_sgd(y_23, X_23, np.ones((30,)), 100, 1e-3)\n",
    "y_pred_0 = get_classification_pred(X_0, w_0)\n",
    "y_pred_1 = get_classification_pred(X_1, w_1)\n",
    "y_pred_23 = get_classification_pred(X_23, w_23)\n",
    "print(f1_score(y_0, y_pred_0))\n",
    "print(f1_score(y_1, y_pred_1))\n",
    "print(f1_score(y_23, y_pred_23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, l = reg_logistic_regression(y_train, x_train_cleaned, 0.2,  np.ones((31,)), 1000, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperParameterTuner(X_0, y_0, 'ridge', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ended lambda 1e-15\n",
      "ended lambda 1.1787686347935866e-14\n",
      "ended lambda 1.389495494373136e-13\n",
      "ended lambda 1.637893706954068e-12\n",
      "ended lambda 1.9306977288832457e-11\n",
      "ended lambda 2.275845926074791e-10\n",
      "ended lambda 2.6826957952797275e-09\n",
      "ended lambda 3.162277660168379e-08\n",
      "ended lambda 3.727593720314938e-07\n",
      "ended lambda 4.393970560760786e-06\n",
      "ended lambda 5.179474679231202e-05\n",
      "ended lambda 0.0006105402296585314\n",
      "ended lambda 0.007196856730011528\n",
      "ended lambda 0.08483428982440726\n",
      "ended lambda 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0006105402296585314, 0.6605847438997582)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb6ad4b3c836f9389b495db494d9cfacf3d61f9be1f3aec1481f7e10dfc9325d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
